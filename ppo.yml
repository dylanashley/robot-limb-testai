algorithm: "ppo"                                  # algorithm to use; "ac" or "brownian" or "ppo"
broadcast: True                                   # if True, then the brain sends a broadcast notification to enable all other devices to know about the brain; if False, then the system relies on other devices broadcasting heartbeats for the brain to know where to send
broadcast_rate_s: 1.0                             # rate at which the broadcast notification is sent if broadcast is True
cameras:                                          # which cameras to use for tag detection
    - "31:33:36:33:37:31"                         # middle joint camera
    - "31:36:31:31:34:31"                         # upper joint outer camera
    - "32:34:30:35:35:31"                         # upper joint inner camera
    - "32:34:31:31:33:33"                         # lower joint camera
display: True                                     # toggles streaming the cameras onto the monitor
dummy_drive: False                                # toggles whether to ignore drive commands for debugging
episode_returns_outfile:
    "results/ppo_episode_returns.npy"             # file to save the episodic return to; will overwrite
event_timeout: 1.0                                # if the receiver is set to display, then this is a timeout on that thread to ensure it does not wait indefinitely if nothing is received
fps_time_range: 2.0                               # used to calculate the rolling average (this is the period in seconds) for frame rate calculations on the display
hyperparameters:                                  # hyperparameters for the ppo algorithm
    anneal_lr: True                               # toggles learning rate annealing for policy and value networks
    clip_coef: 0.2                                # the surrogate clipping coefficient
    clip_vloss: True                              # toggles whether or not to use a clipped loss for the value function
    cuda: True                                    # if toggled, cuda will be enabled by default
    ent_coef: 0.0                                 # coefficient of the entropy
    gae_lambda: 0.95                              # the lambda for the general advantage estimation
    gamma: 0.9                                    # the discount factor gamma
    learning_rate: 0.1                            # the learning rate of the optimizer
    max_grad_norm: 0.5                            # the maximum norm for the gradient clipping
    max_steps: 5                                  # the number of steps to run in each environment per policy rollout
    norm_adv: True                                # toggles advantages normalization
    num_envs: 15                                  # the number of environments to run in each iteration
    num_minibatches: 1                            # the number of mini-batches
    target_kl: null                               # the target KL divergence threshold
    total_timesteps: 3600                         # total timesteps of the experiments
    update_epochs: 5                              # the K epochs to update the policy
    vf_coef: 0.5                                  # coefficient of the value function
limits:                                           # safe servo ranges
    "31:33:36:33:37:31":                          # finger servo; middle joint camera
        min: -1.0
        max: 1.0
    "31:36:31:31:34:31":                          # inner left/right servo; upper joint outer camera
        min: -0.75
        max: 0.75
    "32:34:30:35:35:31":                          # up/down servo; upper joint inner camera
        min: -1.0
        max: 1.0
    "32:34:31:31:33:33":                          # outer left/right servo; lower joint camera
        min: -1.0
        max: 1.0
log_level: "INFO"                                 # how verbose the logging should be
log_verbose: False                                # if set to true, then a lot more logging beyond INFO and DEBUG is logged; log levels do not have VERBOSE
model_outfile:
    "results/ppo_model.pt"                        # file to save the model to; will overwrite
port:
    name: "eno1"
    scapy: False                                  # set to True for windows and False for ubuntu
    type: "ethernet"                              # "serial" or "ethernet" or "router" if using simulated router for connecting to Gazebo sim
process: True                                     # set to False for windows and True for ubuntu (whether multiprocess or multithreaded)
queue_size: 100                                   # used for response queue (whether commands were sent) and buffer queue (buffered messages from robot); set to 0 for unlimited
random_start_location: False                      # toggles episodes starting in zero position or a random location
save_episode_returns: True                        # toggles saving the episodic return
save_model: True                                  # toggles saving the final model
save_step_counts: True                            # toggles saving the steps per episode
servos:                                           # which servos to allow control of
    - "31:33:36:33:37:31"                         # finger servo
    - "31:36:31:31:34:31"                         # inner left/right servo
    - "32:34:30:35:35:31"                         # up/down servo
    - "32:34:31:31:33:33"                         # outer left/right servo
set_slew_rate: False                              # toggles writing the slew rate to each servo at the start
single_thread_interface: False
sleep: 2                                          # time to sleep for between actions
slew_rate: 2                                      # controls how fast the servos can move
step_counts_outfile:
    "results/ppo_step_counts.npy"                 # file to save the steps per episode to; will overwrite
wandb: True                                       # toggles logging to wandb
